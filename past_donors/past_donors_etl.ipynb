{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Concat data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_valid_record(df):\n",
    "    '''Find the first row where all columns are null, and return the row previous to that.'''\n",
    "\n",
    "    last_record_idx = df.isnull().apply(lambda x: all(x), axis=1).idxmax() - 1\n",
    "    \n",
    "    return last_record_idx\n",
    "\n",
    "\n",
    "def process_year_donations(main_fname: str, year:str, colnames: list):\n",
    "    '''Process data from each year and assign standard structure.'''\n",
    "\n",
    "    df = pd.read_excel(main_fname, sheet_name = year)\n",
    "\n",
    "    # we deduce if the column headers are in the first or second row. If necessary, we correct the read of the file\n",
    "    c = 0\n",
    "    for i in df.columns[:20]:\n",
    "        if 'Unnamed:' in str(i):\n",
    "            # if too many Unnamed columns, then we have to skip the first row\n",
    "            c += 1\n",
    "    if c > 8 :\n",
    "        df = pd.read_excel(main_fname, sheet_name = year, header = 1)\n",
    "\n",
    "    last_record_idx = find_last_valid_record(df) # keep only rows containing year data\n",
    "    df = df.iloc[:last_record_idx]\n",
    "\n",
    "    df = df[df.columns & colnames].copy()\n",
    "\n",
    "    if 'Prize/Donation details' in df.columns:\n",
    "        df['Donation details'] = df['Prize/Donation details']\n",
    "    else:\n",
    "        df['Donation details'] = df['Sponsorship level']\n",
    "    \n",
    "    df = df.drop(columns = ['Sponsorship level'\t,'Prize/Donation details'], errors = 'ignore')\n",
    "    df = df.rename(columns = {'Actual \\n$ Amount':'$ Amount'})\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 (80, 5)\n",
      "2011 (86, 5)\n",
      "2012 (84, 5)\n",
      "2013 (93, 5)\n",
      "2014 (140, 5)\n",
      "2015 (149, 5)\n",
      "2016 (133, 5)\n",
      "2017 (103, 5)\n",
      "2018 (77, 5)\n",
      "2019 (56, 5)\n",
      "2020 (29, 5)\n",
      "2021 (33, 5)\n",
      "2022 (15, 5)\n"
     ]
    }
   ],
   "source": [
    "fname = \"../Data/corporate sponsors 2022.xlsx\"\n",
    "#df = pd.read_excel(fname, sheet_name = '2010')\n",
    "\n",
    "colnames = [ 'Added/   Modified', \t'Company Name',\t'Sponsorship level'\t,\n",
    "             '$ Amount', 'Actual \\n$ Amount',\t'Money/Prize Received',  'Prize/Donation details' ]\n",
    "\n",
    "years = [str(i) for i in range(2010,2023,1)]\n",
    "\n",
    "df_concat = process_year_donations(fname, years[0], colnames)[:0] # we just take the structure\n",
    "\n",
    "for y in years:\n",
    "    df_ = process_year_donations(fname, y, colnames)\n",
    "    print(y , df_.shape )\n",
    "    df_['Year'] = y \n",
    "    df_concat = pd.concat( [df_concat, df_])\n",
    "\n",
    "df_concat = df_concat.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Column Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_concat.columns.to_list()\n",
    "new_cols=[]\n",
    "for col in cols:\n",
    "    \n",
    "    col = re.sub(' +', ' ', col)\n",
    "    col = col.replace(' ','_')\n",
    "    col = col.replace('/','')\n",
    "    col = col.replace('$','dollar')\n",
    "    col = col.lower()\n",
    "    new_cols.append(col)\n",
    "\n",
    "df_concat.columns = new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.added_modified = df_concat.added_modified.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find the dates that have a standard format\n",
    "date_format_flg = (df_concat.added_modified.str[4:5] == '-') & (df_concat.added_modified.str[7:8] == '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2010 dates need reformatting as they don't include the year\n",
    "df_concat['2010_day'] = df_concat[date_format_flg & (df_concat.year=='2010')].added_modified.str[2:4]\n",
    "df_concat['2010_month'] = df_concat[date_format_flg & (df_concat.year=='2010')].added_modified.str[5:7]\n",
    "\n",
    "df_concat.loc[date_format_flg & (df_concat.year=='2010'), 'added_modified'] = \\\n",
    "df_concat[date_format_flg & (df_concat.year=='2010')]['year'] + '-'\\\n",
    "+ df_concat[date_format_flg & (df_concat.year=='2010')]['2010_month'] + '-' \\\n",
    "+ df_concat[date_format_flg & (df_concat.year=='2010')]['2010_day'] \n",
    "\n",
    "df_concat = df_concat.drop(columns= [ '2010_day', '2010_month'])\n",
    "\n",
    "df_concat['added_modified'] = pd.to_datetime(df_concat.added_modified, errors = 'coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean some rows that are all empty\n",
    "df_concat = df_concat [ ~df_concat[['company_name', 'dollar_amount','moneyprize_received','donation_details']].isna().all(1) ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dollar amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['dollar_amount_float'] = pd.to_numeric(df_concat['dollar_amount'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 rows were coerced as Null for not having convertable values\n"
     ]
    }
   ],
   "source": [
    "n_coerced = df_concat.loc[df_concat.dollar_amount_float != df_concat.dollar_amount, ['dollar_amount_float','dollar_amount']].shape[0]\n",
    "print(n_coerced, 'rows were coerced as Null for not having convertable values')\n",
    "\n",
    "df_concat.dollar_amount_float = df_concat.dollar_amount_float.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_companies = df_concat.company_name.value_counts().reset_index().rename(columns = {'index':'company', 'company_name':'q_rows'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_companies['company'] = df_companies['company'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "\n",
    "stopwords = stopwords.words('french') + stopwords.words('english') + ['/' ,'-', 'pour', 'ou', 'du', 'la', 'ou', 'de',\n",
    "         'avec', 'par', 'depuis','bien','a', 'd\\'un', '']\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.upper()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    cleantext = unidecode.unidecode(cleantext)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 0 if not w in stopwords] #french stopwords\n",
    "    return \" \".join(filtered_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_companies['company_v2']= df_companies['company'].apply(lambda x: preprocess(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_companies = df_companies[df_companies.company_v2 != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV with company name pairs that should be unified\n",
    "\n",
    "similar_companies = pd.read_csv('../Data/similar_companies.csv', encoding = 'utf-8')\n",
    "similar_companies = similar_companies[ similar_companies.iloc[:,2] == 1 ].iloc[:,:2]\n",
    "similar_companies['lens'] =  similar_companies.apply(lambda x:  [len(x[0]) , len(x[1]) ] , axis = 1)\n",
    "\n",
    "similar_companies['id_shortest_name'] = similar_companies.lens.apply(lambda x: x.index(min(x))) # choose which name to keep as main\n",
    "\n",
    "similar_companies['shortest_name'] = \\\n",
    "    similar_companies.apply(lambda x:  x[0] if x['id_shortest_name'] == 0 else x[1], axis=1)\n",
    "\n",
    "similar_companies = similar_companies.iloc[:,[0,1,4]]\n",
    "similar_companies.columns = ['name_a', 'name_b', 'shortest_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propagate the main name back to the main df\n",
    "\n",
    "df_companies = df_companies.merge(similar_companies[['name_a','shortest_name']],\n",
    "         how='left',\n",
    "         left_on = 'company_v2', \n",
    "         right_on='name_a')\n",
    "\n",
    "df_companies = df_companies.merge(similar_companies[['name_b','shortest_name']],\n",
    "         how='left',\n",
    "         left_on = 'company_v2', \n",
    "         right_on='name_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_companies['company_v3'] = df_companies['company_v2'].copy()\n",
    "\n",
    "df_companies.loc[ df_companies.shortest_name_x.notna(), 'company_v3'] =  df_companies.shortest_name_x\n",
    "\n",
    "df_companies.loc[ df_companies.shortest_name_y.notna(), 'company_v3'] =  df_companies.shortest_name_y\n",
    "\n",
    "df_companies = df_companies[['company', 'company_v3']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the company names back to the main df_concat\n",
    "\n",
    "df_concat = df_concat.merge(df_companies, how = 'left', left_on='company_name', right_on = 'company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_concat[['company_v3', 'added_modified', 'dollar_amount_float','moneyprize_received', 'donation_details', 'year']]\n",
    "\n",
    "df = df.rename(columns = {'company_v3': 'company'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$200 1/4 page ad                                58\n",
       "business card                                   53\n",
       "$500 donation                                   44\n",
       "$100 business card                              34\n",
       "business card ad                                33\n",
       "Corporate team                                  31\n",
       "$1000 donation                                  22\n",
       "$300 1/2 page ad                                21\n",
       "1/4 page ad                                     19\n",
       "Silver                                          18\n",
       "K4K Friend                                      15\n",
       "1/2 page ad                                     14\n",
       "$100 Busines card                               11\n",
       "Thank you page                                   9\n",
       "$500 K4K friend                                  8\n",
       "reduced accounting fees                          8\n",
       "Roundtrip for two to any WestJet destination     8\n",
       "Longtime sponsor                                 7\n",
       "$500 Silver Sponsor                              7\n",
       "thank you sponsor                                7\n",
       "Name: donation_details, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.donation_details.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['don_detail_amount'] = df['donation_details'].str.extract('(\\$[0-9,.]+)')\n",
    "df['don_detail_amount'] = pd.to_numeric(df['don_detail_amount'].str.replace('$', '').str.replace(',', ''))\n",
    "\n",
    "# lets create a column that joins the amount information from donation or equivalents\n",
    "df['dollar_equivalent_amount'] = df.dollar_amount_float\n",
    "df.loc[ (df.dollar_amount_float == 0) & (df.don_detail_amount > 0)  ,['dollar_equivalent_amount'] ] = df.don_detail_amount\n",
    "\n",
    "# df[['dollar_equivalent_amount', 'dollar_amount_float']].describe() we see the new column has more info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean donation detail of any amounts, just keep the description in a new col\n",
    "df['don_detail_txt']=df['donation_details'].str.replace('(\\$[0-9,.]+)','', n=1)\n",
    "df['don_detail_txt'].replace(r'\\s+',' ', regex=True, inplace=True)\n",
    "df['don_detail_txt'] = df['don_detail_txt'].str.lower()\n",
    "df['don_detail_txt'] = df['don_detail_txt'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('past_donors_clean.csv', index=None)\n",
    "df.to_pickle('past_donors_clean.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e76f8642360a3358ab06c318ab18c1161224eb3836609d5c16edea6f6e43dba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
